{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import random as rd\n",
    "from collections import Counter\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# inspired by https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preproc(text):\n",
    "    'preprocessing of large text string'\n",
    "    text = re.sub(' Mr.', ' Mr', text)\n",
    "    text = re.sub(' Mrs.', ' Mrs', text)\n",
    "    text = re.sub(' Messrs.', ' Messrs', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[(.*?)\\]', '', text)\n",
    "    text = re.sub('\\n\\n', ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub(',', '', text)\n",
    "    text = re.sub('_', '', text)\n",
    "    text = re.sub(';', '', text)\n",
    "    text = re.sub('_figure_', '', text)\n",
    "    text = re.sub('\\d+','',text)\n",
    "    return text\n",
    "\n",
    "# get all words in target text file, plus all files in a directory for optional pre-training\n",
    "def get_data(targetname,pretrains = None):\n",
    "    'load text from file, optional pretraining'\n",
    "    rawwords=[]\n",
    "    if pretrains: # optional\n",
    "        filename = glob.glob(pretrains) # get directory and file ending for pretraining files\n",
    "        count=0\n",
    "        for i in filename:\n",
    "            text = open(i).read()\n",
    "            text = preproc(text) # preprocess file text\n",
    "            wds = re.findall(r\"[\\w']+|[.,!?;]\", text) # split text into individual words\n",
    "            for wd in wds:\n",
    "                rawwords.append(wd) # append individual words to our raw training array\n",
    "            count +=1\n",
    "            print(\"loading file...\", count+1, \"/\",len(filename),\" :: \",i)\n",
    "    print('Words in pretraining set: ', len(rawwords))\n",
    "    text = open(targetname).read() # now do the same for target file\n",
    "    text = preproc(text)\n",
    "    wds = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    print('Words in target set: ', len(wds))\n",
    "    for wd in wds:\n",
    "        rawwords.append(wd)\n",
    "    return rawwords\n",
    "\n",
    "def predict_words(n_pred, words, model, int_to_word, word_to_int, seq_length):\n",
    "    'predict words given a model'\n",
    "    start = np.random.randint(0, len(words)-seq_length) # pick a random seed\n",
    "    pattern = words[start:start+seq_length] # get a full sequence\n",
    "    print(\"Seed: \",\" \".join(pattern))\n",
    "    pattern = [word_to_int[pat] for pat in pattern] # turn to ints for model\n",
    "    print(\"Prediction: \")\n",
    "    for i in range(n_pred): # for n_pred words (length of predicted sequence)\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1)) # make data pretty for model\n",
    "        x = x / float(n_words)\n",
    "        prediction = model.predict(x, verbose=0) # get model predictions (probabilities of unique words)\n",
    "        index = np.random.multinomial(1, np.squeeze(prediction)) # sample over word probabilities to get actual prediction\n",
    "        result = int_to_word[list(index).index(1)]\n",
    "        seq_in = [int_to_word[value] for value in pattern]\n",
    "        while result=='RARE': # if model predicts rare, sample again until it finds a more frequent word\n",
    "            index = np.random.multinomial(1, np.squeeze(prediction))\n",
    "            result = int_to_word[list(index).index(1)]\n",
    "        sys.stdout.write(result) # print result\n",
    "        sys.stdout.write(\" \")\n",
    "        pattern.append(list(index).index(1))\n",
    "        pattern = pattern[1:len(pattern)] # delete first element of pattern and continue (slowly gets rid of seed)\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load (pre-)training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rawwords = get_data('gwtext.txt','pretrain/*.txt')\n",
    "\n",
    "# get word counts and dictionaries and make a category for rare words\n",
    "word_counts = Counter(word for word in rawwords)\n",
    "words = [ word if word_counts[word]>3 else 'RARE' for word in rawwords ]\n",
    "unique_words = sorted(list(set(words)))\n",
    "word_to_int = dict((c, i) for i, c in enumerate(unique_words))\n",
    "int_to_word = dict((i, c) for i, c in enumerate(unique_words))\n",
    "\n",
    "# get number of words\n",
    "n_words = len(words)\n",
    "n_uwords = len(unique_words)\n",
    "print('Total Words (without rare words): ', n_words)\n",
    "print('Unique Words (without rare words): ', n_uwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Make training batch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batchsize=150\n",
    "seq_length=20\n",
    "batches = int((n_words-seq_length)/batchsize) # rounded down\n",
    "savedir = \"traindat\"\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for bat in range(0,batches): # for each batch of words, roll a seq_length window over the words to create individual training samples\n",
    "    print(\"Batch \",bat+1,\" out of \",batches)\n",
    "    X = np.zeros([batchsize, seq_length, 1])\n",
    "    dataY = np.zeros([batchsize, 1])\n",
    "    batcount = 0\n",
    "    for i in range(count, count + batchsize):\n",
    "        seq_in = words[i:i + seq_length] # seq_length number of words from all training words\n",
    "        seq_out = words[i + seq_length] # the word after that, training signal word 21\n",
    "        X[batcount,:,0]=[word_to_int[word] for word in seq_in] # seq of words turned into int value\n",
    "        dataY[batcount,0]=word_to_int[seq_out] # training signal word words turned into int value\n",
    "        batcount+=1\n",
    "    X = X / np.float32(n_uwords) # normalize\n",
    "    y = np_utils.to_categorical(dataY,num_classes=n_uwords) # one hot encoder\n",
    "    f = savedir+\"/\"+\"train\"+str(bat)+\".h5\" # file name for batch, then save all\n",
    "    h5f = h5py.File(f, 'w')\n",
    "    h5f.create_dataset('X', data=X)\n",
    "    h5f.create_dataset('y', data=y)\n",
    "    h5f.close()\n",
    "    count += batchsize # move one batch forward and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Done!\", batches,\"batches of size\", batchsize, \"(sequences of\",seq_length,\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_length, 1), return_sequences=True, recurrent_dropout=0.1,dropout=0.2))\n",
    "model.add(LSTM(256, input_shape=(seq_length, 1), return_sequences=True, recurrent_dropout=0.1,dropout=0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(n_uwords, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# (Pre-)train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "episodes=1 # \n",
    "fromdir = \"traindat\"\n",
    "eploss = []\n",
    "for ep in range(0,episodes):\n",
    "    print(\"Episode \",ep+1,\" out of \",episodes, \"...\")\n",
    "    count = 0\n",
    "    batarr = list(range(0,batches))\n",
    "    rd.shuffle(batarr)\n",
    "    batloss = []\n",
    "    for bat in batarr:\n",
    "        # load ep\n",
    "        f = fromdir+\"/\"+\"train\"+str(bat)+\".h5\"\n",
    "        h5f = h5py.File(f, 'r')\n",
    "        X = h5f['X'][:]\n",
    "        y = h5f['y'][:]\n",
    "        h5f.close()\n",
    "        loss = model.train_on_batch(X, y)\n",
    "        batloss.append(loss)\n",
    "        print(\"Batch \",count,\" out of \",batches,\"(\",bat,\")\",\" :: loss: \",loss)\n",
    "        count +=1\n",
    "    eploss.append(np.mean(batloss))\n",
    "    modn='weights/model'+str(ep+1)+'.h5'\n",
    "    model.save(modn)\n",
    "    print(\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Predictions after (pre-)training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = predict_words(40, words, model, int_to_word, word_to_int,seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Get main training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get target text and make a category for rare words\n",
    "rawwords = get_data(10,'gwtext.txt')\n",
    "# get word counts and dictionaries and make a category for rare words\n",
    "word_counts = Counter(word for word in rawwords)\n",
    "words = [ word if word_counts[word]>3 else 'RARE' for word in rawwords ]\n",
    "unique_words = sorted(list(set(words)))\n",
    "word_to_int = dict((c, i) for i, c in enumerate(unique_words))\n",
    "int_to_word = dict((i, c) for i, c in enumerate(unique_words))\n",
    "\n",
    "# get number of words\n",
    "n_words = len(words)\n",
    "n_uwords = len(unique_words)\n",
    "print('Total Words (without rare words): ', n_words)\n",
    "print('Unique Words (without rare words): ', n_uwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_words-seq_length, 1):\n",
    "    seq_in = words[i:i + seq_length] # seq_length number of words from all training words\n",
    "    seq_out = words[i + seq_length] # the word after that, training signal\n",
    "    dataX.append([word_to_int[word] for word in seq_in]) # seq of words turned into int value\n",
    "    dataY.append(word_to_int[seq_out]) # training signal word words turned into int value\n",
    "n_patterns = len(dataX)\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1)) # [individual samples, length of sequence, features]\n",
    "X = X / np.float32(n_uwords) # normalize\n",
    "y = np_utils.to_categorical(dataY,num_classes=n_uwords) # one hot encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove last layer and retrain on only the target file\n",
    "model.pop()\n",
    "model.add(Dense(n_uwords, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"washington-weight-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=5, batch_size=200, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = load_model('washington-weight-03-6.0754.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_words(100, words, model, int_to_word, word_to_int,seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
